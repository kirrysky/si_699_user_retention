{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree.export import export_graphviz\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "data=data.drop('overall_time_mean_na',1)\n",
    "data=data.drop('overall_time_min_na',1)\n",
    "data=data.drop('overall_time_max_na',1)\n",
    "\n",
    "\n",
    "\n",
    "features=pd.read_csv('feature_rank.csv')\n",
    "\n",
    "cols=features['features'].tolist()\n",
    "\n",
    "cols=cols[:70]\n",
    "\n",
    "cols.append('label')\n",
    "\n",
    "data=data[cols]\n",
    "\n",
    "#data=data[data.columns.drop(list(data.filter(regex='log')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test=pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "data_test_original=data_test.drop('overall_time_mean_na',1)\n",
    "data_test_original=data_test.drop('overall_time_min_na',1)\n",
    "data_test_original=data_test.drop('overall_time_max_na',1)\n",
    "\n",
    "\n",
    "\n",
    "features=pd.read_csv('feature_rank.csv')\n",
    "\n",
    "cols=features['features'].tolist()\n",
    "\n",
    "cols=cols[:70]\n",
    "\n",
    "cols.append('label')\n",
    "\n",
    "data_test_original=data_test_original[cols]\n",
    "\n",
    "#data_test=data_test[data_test.columns.drop(list(data_test.filter(regex='log')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_original=data.drop(['label'],axis=1)\n",
    "\n",
    "y_train_original = data['label']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_original=data_test_original.drop(['label'],axis=1)\n",
    "\n",
    "y_test_original=data_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Standardizing the features\n",
    "# x = StandardScaler().fit_transform(X1)\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA \n",
    "\n",
    "# #Explained variance\n",
    "# pca = PCA().fit(x)\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('cumulative explained variance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=15)\n",
    "# pca_train=pca.fit(x)\n",
    "\n",
    "# pca_train_features=pca_train.transform(x)\n",
    "\n",
    "# np.asmatrix(pca_train_features)[:,14]\n",
    "\n",
    "# for i in range(0,15):\n",
    "#     X1['pca'+str(i)]=np.asmatrix(pca_train_features)[:,i]\n",
    "\n",
    "# X1.shape\n",
    "\n",
    "# pca_test_features=pca_train.transform(StandardScaler().fit_transform(X_test))\n",
    "\n",
    "# for i in range(0,15):\n",
    "#     X_test['pca'+str(i)]=np.asmatrix(pca_test_features)[:,i]\n",
    "\n",
    "# X_test.shape\n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn import metrics\n",
    "# from scipy.spatial.distance import cdist\n",
    "\n",
    "# # create new plot and data\n",
    "# plt.plot()\n",
    "# sc = StandardScaler()\n",
    "# X = x\n",
    "\n",
    "\n",
    "# # k means determine k\n",
    "# distortions = []\n",
    "# K = range(1,10)\n",
    "# for k in K:\n",
    "#     kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "#     kmeanModel.fit(X)\n",
    "#     distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# # Plot the elbow\n",
    "# plt.plot(K, distortions, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Distortion')\n",
    "# plt.title('The Elbow Method showing the optimal k')\n",
    "# plt.show()\n",
    "\n",
    "# kmeans = KMeans(n_clusters=4, random_state=0).fit(x)\n",
    "\n",
    "# X1['cluster']=kmeans.labels_\n",
    "\n",
    "# X_test['cluster']=kmeans.predict(StandardScaler().fit_transform(X_test[X_test.columns.drop(list(X_test.filter(regex='pca')))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataX={'all features':X_train_original}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "all features\n",
      "1\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "C ------- 0.1\n",
      "[0. 1.]\n",
      "C ------- 10\n",
      "[0. 1.]\n",
      "2\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "C ------- 0.1\n",
      "[0. 1.]\n",
      "C ------- 10\n",
      "[0. 1.]\n",
      "3\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "C ------- 0.1\n",
      "[0. 1.]\n",
      "C ------- 10\n",
      "[0. 1.]\n",
      "4\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "C ------- 0.1\n",
      "[0. 1.]\n",
      "C ------- 10\n",
      "[0. 1.]\n",
      "5\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "C ------- 0.1\n",
      "[0. 1.]\n",
      "C ------- 10\n",
      "[0. 1.]\n",
      "train 0.8822266951560456\n",
      "test 0.8723596482971656\n",
      "train_scale 0.8822108053299169\n",
      "test_scale 0.8723943708376949\n",
      "train 0.8821266102521881\n",
      "test 0.8723525010204236\n",
      "train_scale 0.8821159947881602\n",
      "test_scale 0.8723958784415642\n",
      "train 0.8822375131385238\n",
      "test 0.8723604869642185\n",
      "train_scale 0.8822133305754596\n",
      "test_scale 0.8723879687402287\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_coe={}\n",
    "for XX in dataX.keys():\n",
    "    print('**********')\n",
    "    print(XX)\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "    sss.get_n_splits(dataX[XX], y_train_original)  \n",
    "    i=0\n",
    "    ptest={}\n",
    "    ptrain={}\n",
    "    ptest_scal={}\n",
    "    ptrain_scal={}\n",
    "    \n",
    "  \n",
    "\n",
    "    for c in [1,0.1,10]:\n",
    "        ptest[c]=0\n",
    "        ptrain[c]=0\n",
    "        ptest_scal[c]=0\n",
    "        ptrain_scal[c]=0\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    for train_index, test_index in sss.split(dataX[XX], y_train_original):\n",
    "        i+=1\n",
    "        print(i)\n",
    "\n",
    "        X_train, X_test = dataX[XX].iloc[train_index], dataX[XX].iloc[test_index]\n",
    "        y_train, y_test = y_train_original[train_index], y_train_original[test_index]\n",
    "            \n",
    "        sm = SMOTE(random_state=12, sampling_strategy = 1.0)\n",
    "        X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "        \n",
    "        ###scale\n",
    "        sc = StandardScaler()\n",
    "            \n",
    "        X_train2= sc.fit_transform(X_train)\n",
    "        X_test2= sc.transform (X_test)\n",
    "       \n",
    "        for c in [1,0.1,10]:\n",
    "\n",
    "            print('C -------',c)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            clf = LogisticRegression(penalty='l1', C=c)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            #clf_predict_test=clf.predict(X_test)\n",
    "\n",
    "            #clf_predict_train=clf.predict(X_train)\n",
    "            \n",
    "            \n",
    "            print(clf.classes_)\n",
    "\n",
    "            \n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_train, clf.predict_proba(X_train)[:,1])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ptrain[c]+=roc_auc/5\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ptest[c]+=roc_auc/5\n",
    "            \n",
    "          \n",
    "            \n",
    "#             coef_dict = {}\n",
    "#             for coef, feat in zip(clf.coef_[0],dataX[XX].columns.tolist()):\n",
    "#                 coef_dict[feat] = coef\n",
    "            \n",
    "#             model_coe[XX+'_'+str(c)+'_noscale']=coef_dict\n",
    "\n",
    "            ###scale\n",
    "            \n",
    "\n",
    "            clf = LogisticRegression(penalty='l1', C=c)\n",
    "            clf.fit(X_train2, y_train)\n",
    "            #clf_predict_test=clf.predict(X_test2)\n",
    "\n",
    "            #clf_predict_train=clf.predict(X_train2)\n",
    "\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_train, clf.predict_proba(X_train2)[:,1])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ptrain_scal[c]+=roc_auc/5\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_test, clf.predict_proba(X_test2)[:,1])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ptest_scal[c]+=roc_auc/5\n",
    "            \n",
    "            \n",
    "#             coef_dict = {}\n",
    "#             for coef, feat in zip(clf.coef_[0],dataX[XX].columns.tolist()):\n",
    "#                 coef_dict[feat] = coef\n",
    "            \n",
    "#             model_coe[XX+'_'+str(c)+'_scale']=coef_dict\n",
    "    for c in [1,0.1,10]:\n",
    "        print('train',ptrain[c])\n",
    "        print('test',ptest[c])\n",
    "        print('train_scale',ptrain_scal[c])\n",
    "        print('test_scale',ptest_scal[c])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "train 0.8822266951560456\n",
      "test 0.8723596482971656\n",
      "train_scale 0.8822108053299169\n",
      "test_scale 0.8723943708376949\n",
      "0.1\n",
      "train 0.8821266102521881\n",
      "test 0.8723525010204236\n",
      "train_scale 0.8821159947881602\n",
      "test_scale 0.8723958784415642\n",
      "10\n",
      "train 0.8822375131385238\n",
      "test 0.8723604869642185\n",
      "train_scale 0.8822133305754596\n",
      "test_scale 0.8723879687402287\n"
     ]
    }
   ],
   "source": [
    "for c in [1,0.1,10]:\n",
    "    print(c)\n",
    "    print('train',ptrain[c])\n",
    "    print('test',ptest[c])\n",
    "    print('train_scale',ptrain_scal[c])\n",
    "    print('test_scale',ptest_scal[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "sm = SMOTE(random_state=12, sampling_strategy = 1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train_original, y_train_original)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', C=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "testprob=clf.predict_proba(X_test_original)[:,1]\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test_original, testprob)\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.872367382490481"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1=pd.DataFrame(columns=['logistic_regression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1['logistic_regression']=testprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1.to_csv('logistic_regression1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "all features\n",
      "1\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "2\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "3\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "4\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "5\n",
      "C ------- 1\n",
      "[0. 1.]\n",
      "train 0.8719822994643202\n",
      "test 0.8731242892927047\n",
      "train_scale 0.8719935621774341\n",
      "test_scale 0.8731328321421865\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_prob=0\n",
    "model_coe={}\n",
    "for XX in dataX.keys():\n",
    "    print('**********')\n",
    "    print(XX)\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "    sss.get_n_splits(dataX[XX], y)  \n",
    "    i=0\n",
    "    ptest={}\n",
    "    ptrain={}\n",
    "    ptest_scal={}\n",
    "    ptrain_scal={}\n",
    "    \n",
    "  \n",
    "\n",
    "    for c in [1]:\n",
    "        ptest[c]=0\n",
    "        ptrain[c]=0\n",
    "        ptest_scal[c]=0\n",
    "        ptrain_scal[c]=0\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    for train_index, test_index in sss.split(dataX[XX], y):\n",
    "        i+=1\n",
    "        print(i)\n",
    "\n",
    "        X_train, X_test = dataX[XX].iloc[train_index], dataX[XX].iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "#         sm = SMOTE(random_state=12, sampling_strategy = 1.0)\n",
    "#         X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "        \n",
    "        ###scale\n",
    "        sc = StandardScaler()\n",
    "            \n",
    "        X_train2= sc.fit_transform(X_train)\n",
    "        X_test2= sc.transform (X_test)\n",
    "       \n",
    "        for c in [1]:\n",
    "\n",
    "            print('C -------',c)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            clf = LogisticRegression(penalty='l1', C=c)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            #clf_predict_test=clf.predict(X_test)\n",
    "\n",
    "            #clf_predict_train=clf.predict(X_train)\n",
    "            \n",
    "            \n",
    "            print(clf.classes_)\n",
    "\n",
    "            \n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_train, clf.predict_proba(X_train)[:,1])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ptrain[c]+=roc_auc/5\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ptest[c]+=roc_auc/5\n",
    "            \n",
    "            final_prob+=clf.predict_proba(X_test)[:,1]/5\n",
    "          \n",
    "            \n",
    "#             coef_dict = {}\n",
    "#             for coef, feat in zip(clf.coef_[0],dataX[XX].columns.tolist()):\n",
    "#                 coef_dict[feat] = coef\n",
    "            \n",
    "#             model_coe[XX+'_'+str(c)+'_noscale']=coef_dict\n",
    "\n",
    "            ###scale\n",
    "            \n",
    "\n",
    "            clf = LogisticRegression(penalty='l1', C=c)\n",
    "            clf.fit(X_train2, y_train)\n",
    "            #clf_predict_test=clf.predict(X_test2)\n",
    "\n",
    "            #clf_predict_train=clf.predict(X_train2)\n",
    "\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_train, clf.predict_proba(X_train2)[:,1])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ptrain_scal[c]+=roc_auc/5\n",
    "\n",
    "            fpr, tpr, threshold = metrics.roc_curve(y_test, clf.predict_proba(X_test2)[:,1])\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            ptest_scal[c]+=roc_auc/5\n",
    "            \n",
    "            \n",
    "#             coef_dict = {}\n",
    "#             for coef, feat in zip(clf.coef_[0],dataX[XX].columns.tolist()):\n",
    "#                 coef_dict[feat] = coef\n",
    "            \n",
    "#             model_coe[XX+'_'+str(c)+'_scale']=coef_dict\n",
    "    for c in [1]:\n",
    "        print('train',ptrain[c])\n",
    "        print('test',ptest[c])\n",
    "        print('train_scale',ptrain_scal[c])\n",
    "        print('test_scale',ptest_scal[c])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob=pd.DataFrame(columns=['lr_prob'])\n",
    "\n",
    "prob['lr_prob']=final_prob\n",
    "\n",
    "prob.to_csv('lr_prob.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
